\chapter{Related work}\label{chp:related}

For the case of applications such as assistant and companion robots, a good human following ability is needed. There are robots which are meant to carry loads of different weights and follow people autonomously. Such robots can be deployed in a variety of fields. There can be a load bearing robot for use alongside first responders, which may bear weights that might hamper the mobility of the person in a disaster response scenarios.  The situation of use of such robots can lead to problems of their own, such as loss of tracking due to presence of static and dynamic obstacles. Such obstacles can cause a loss of target object. The challenged posed by such an application are: detection of human, tracking of the human and the successful following of that human. The path of the robot is based on the path of the target being followed. The robot must also navigate the environment successfully, avoiding collisions.  Many works have been published in the field of person tracking with robots \cite{article}. Many of these implementations are based on laser range finder data as many models of autonomous robots are equipped with LRF's. The LRF data is presented as 2D image. Also as the LRF sensors are usually placed at a lower height most of these tracking methods use the geometric data of legs to track humans. There exist implementations \cite{KAWARAZAKI2015455}\cite{7342938} of person tracking through laser itself, but it is not very reliable in a cluttered environment.

The paper published by Chebotareva Et. al. \cite{inbook}. deals presents a human-following algorithm for an autonomous robot, which has a monocular camera and a 2D Laser Range finder. The use case for both this project and the one mentioned in \cite{inbook} is an indoor, office/university environment. Usually, the head and upper body is used for this form of object tracking, but this paper proposes a method which does not need to have the full human figure in frame at initialization and can track based on the lower body and does not place restrictions on the nature of clothing worn. Their implementation was compared by simulating experiments in Gazebo for the use cases of a corridor and office/lab room environment.
 
Their method was to take the LRF data at initialization, the human to be tracked is closest to the robot. These are a collection of points, which are connected to the nearing points to create a curve. This approach allowed them to record the position of the legs and track it easily. Several trackers were deployed such as: KCF, TLD, Median Flow, MOSSE and MIL. The most promising results of the LRF based tracking were from MIL and MOSSE. Even though all the used trackers had their shortcomings, to overcome this, they supported it using the data from a monocular camera. 


Another approach is a method like one mentioned in \cite[]{app12031319}. Their implementation of Multi-Object-Tracking uses deep learning, DeepSort. In the MOT algorithm, simple online and real-time tracking (SORT) adopts the Kalman filter and Hungarian matching algorithm to track the objects, which obtains fast and great tracking performance. 
